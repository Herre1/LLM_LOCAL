# ğŸ’¬ Chat con Gemma 1B - LLM Local

Una aplicaciÃ³n web moderna para chatear con modelos de lenguaje locales usando Ollama y Flask.

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Prerrequisitos

1. **Python 3.8+** instalado en tu sistema
2. **Ollama** ejecutÃ¡ndose localmente en el puerto 11434
3. **Modelo Gemma 3 1B** descargado en Ollama

### Paso 1: Instalar Ollama

```bash
ollama pull gemma3:1b
```

### Paso 2: Clonar el proyecto

```bash
git clone <tu-repositorio>
cd LLM_LOCAL
```

### Paso 3: Instalar dependencias

```bash
pip install flask requests
```

### Paso 4: Ejecutar la aplicaciÃ³n

```bash
python app.py
```

La aplicaciÃ³n estarÃ¡ disponible en: **http://127.0.0.1:5000**

## ğŸ“ Estructura del Proyecto

```
LLM_LOCAL/
â”œâ”€â”€ app.py                 # AplicaciÃ³n principal de Flask
â”œâ”€â”€ ollama_client.py       # Cliente para comunicarse con Ollama
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html        # Interfaz web del chat
â””â”€â”€ README.md             # Este archivo
```

## ğŸ› ï¸ Uso

### Chat Normal
1. Escribe tu pregunta en el campo de texto
2. Presiona **"Enviar"** o Enter
3. Espera la respuesta del modelo Gemma

### Frases Motivacionales
- Haz clic en **"âœ¨ MotivaciÃ³n"** para recibir una frase inspiradora al instante

### Limpiar Historial
- Usa el botÃ³n **"Limpiar"** para borrar todo el historial del chat

## âš™ï¸ ConfiguraciÃ³n

### Cambiar el Modelo

Edita el archivo `ollama_client.py`:

```python
MODEL = "gemma2:2b"  # Cambia por tu modelo preferido
```

### Personalizar Puerto

Modifica el archivo `app.py`:

```python
app.run(port=5000, debug=True)  # Cambia el puerto aquÃ­
```

### Ajustar LÃ­mite de Historial

En `app.py`, lÃ­nea 38:

```python
session["historial"] = session["historial"][-20:]  # MÃ¡ximo 20 mensajes
```