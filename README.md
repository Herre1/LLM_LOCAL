# ğŸ’¬ Chat con Gemma 2B - LLM Local

Una aplicaciÃ³n web moderna para chatear con modelos de lenguaje locales usando Ollama y Flask.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Flask](https://img.shields.io/badge/Flask-2.0+-green.svg)
![Ollama](https://img.shields.io/badge/Ollama-Compatible-orange.svg)

## ğŸŒŸ CaracterÃ­sticas

- **ğŸ’» Chat en tiempo real** con modelos LLM locales
- **âœ¨ Frases motivacionales** con un solo clic
- **ğŸ¨ Interfaz moderna** con tema oscuro
- **ğŸ“± DiseÃ±o responsivo** que se adapta a cualquier pantalla
- **ğŸ§¹ Historial limpio** con funciÃ³n de borrado
- **âš¡ Arquitectura modular** y cÃ³digo bien organizado
- **ğŸ”’ Privacidad total** - todo funciona localmente

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Prerrequisitos

1. **Python 3.8+** instalado en tu sistema
2. **Ollama** ejecutÃ¡ndose localmente en el puerto 11434
3. **Modelo Gemma 2B** descargado en Ollama

### Paso 1: Instalar Ollama

```bash
# Descargar Ollama desde: https://ollama.ai
# Luego instalar el modelo Gemma 2B:
ollama pull gemma2:2b
```

### Paso 2: Clonar el proyecto

```bash
git clone <tu-repositorio>
cd LLM_LOCAL
```

### Paso 3: Instalar dependencias

```bash
pip install flask requests
```

### Paso 4: Ejecutar la aplicaciÃ³n

```bash
python app.py
```

La aplicaciÃ³n estarÃ¡ disponible en: **http://127.0.0.1:5000**

## ğŸ“ Estructura del Proyecto

```
LLM_LOCAL/
â”œâ”€â”€ app.py                 # AplicaciÃ³n principal de Flask
â”œâ”€â”€ ollama_client.py       # Cliente para comunicarse con Ollama
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html        # Interfaz web del chat
â””â”€â”€ README.md             # Este archivo
```

## ğŸ› ï¸ Uso

### Chat Normal
1. Escribe tu pregunta en el campo de texto
2. Presiona **"Enviar"** o Enter
3. Espera la respuesta del modelo Gemma

### Frases Motivacionales
- Haz clic en **"âœ¨ MotivaciÃ³n"** para recibir una frase inspiradora al instante

### Limpiar Historial
- Usa el botÃ³n **"Limpiar"** para borrar todo el historial del chat

## âš™ï¸ ConfiguraciÃ³n

### Cambiar el Modelo

Edita el archivo `ollama_client.py`:

```python
MODEL = "gemma2:2b"  # Cambia por tu modelo preferido
```

### Personalizar Puerto

Modifica el archivo `app.py`:

```python
app.run(port=5000, debug=True)  # Cambia el puerto aquÃ­
```

### Ajustar LÃ­mite de Historial

En `app.py`, lÃ­nea 38:

```python
session["historial"] = session["historial"][-20:]  # MÃ¡ximo 20 mensajes
```

## ğŸ¨ PersonalizaciÃ³n de la Interfaz

### Colores del Tema

Edita `templates/index.html` para cambiar los colores:

```css
body { 
    background-color: #1e1f26;  /* Fondo principal */
    color: #f5f6fa;             /* Texto principal */
}

.chat-box {
    background: #2a2d38;        /* Fondo del chat */
}

.usuario { 
    background: #4f8cff;        /* Mensajes del usuario */
}

.asistente { 
    background: #3b3f4c;        /* Mensajes del asistente */
}
```

### Agregar Nuevas Frases Motivacionales

En `app.py`, modifica la lista `PROMPTS_MOTIVACIONALES`:

```python
PROMPTS_MOTIVACIONALES = [
    "Dame una frase motivacional corta",
    "InspÃ­rame con una frase positiva",
    "Tu nueva frase aquÃ­",  # Agrega mÃ¡s aquÃ­
]
```

## ğŸ”§ SoluciÃ³n de Problemas

### Error: "Ollama no estÃ¡ corriendo"
```bash
# Verificar que Ollama estÃ© ejecutÃ¡ndose:
ollama serve
```

### Error: "Modelo no encontrado"
```bash
# Instalar el modelo necesario:
ollama pull gemma2:2b
```

### Error de importaciÃ³n
```bash
# Limpiar cachÃ© de Python:
rm -rf __pycache__/
```

### Puerto ocupado
- Cambia el puerto en `app.py` a otro disponible (ej: 5001, 8000)

## ğŸ†š Ventajas sobre Otros Proyectos

âœ… **Arquitectura modular** - CÃ³digo organizado y mantenible  
âœ… **Templates separados** - HTML no embebido en Python  
âœ… **MÃºltiples funcionalidades** - Chat + motivaciÃ³n + limpieza  
âœ… **Mejor manejo de errores** - Experiencia de usuario robusta  
âœ… **DiseÃ±o Ãºnico** - Interfaz personalizada y atractiva  
âœ… **DocumentaciÃ³n completa** - README detallado  

## ğŸ¤ Contribuciones

Â¡Las contribuciones son bienvenidas! SiÃ©ntete libre de:

1. Hacer fork del proyecto
2. Crear una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'Agrega nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crear un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo `LICENSE` para mÃ¡s detalles.

## ğŸ‘¨â€ğŸ’» Autor

Desarrollado con â¤ï¸ para la comunidad de IA local.

---

### ğŸš€ Â¡Disfruta chateando con tu IA local!

Si te gusta este proyecto, Â¡dale una â­ en GitHub!
